{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chappelle Monologue Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Jason Brownlee's Machine Learning Mastery: \"How to Develop a Word-Level Neural Language Model and Use it to Generate Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcripts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good people of Atlanta, we must never forget… that\n"
     ]
    }
   ],
   "source": [
    "print(transcript_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good people of Atlanta, we must never forget… that\n",
      "['good', 'people', 'of', 'atlanta', 'we', 'must', 'never', 'that', 'anthony', 'yeah', 'himself', 'anthony', 'bourdain', 'had', 'the', 'greatest', 'job', 'that', 'show', 'business']\n",
      "Total Tokens: 52922\n",
      "Unique Tokens: 4313\n",
      "Total Sequences: 52871\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"utf-8\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'transcripts.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:50])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'trans_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            488200    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9764)              986164    \n",
      "=================================================================\n",
      "Total params: 1,625,264\n",
      "Trainable params: 1,625,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 147278 samples\n",
      "Epoch 1/100\n",
      "147278/147278 [==============================] - 24s 163us/sample - loss: 6.4986 - accuracy: 0.0409\n",
      "Epoch 2/100\n",
      "147278/147278 [==============================] - 21s 140us/sample - loss: 6.0774 - accuracy: 0.0629\n",
      "Epoch 3/100\n",
      "147278/147278 [==============================] - 21s 141us/sample - loss: 5.8531 - accuracy: 0.0834\n",
      "Epoch 4/100\n",
      "147278/147278 [==============================] - 21s 140us/sample - loss: 5.6982 - accuracy: 0.0960\n",
      "Epoch 5/100\n",
      "147278/147278 [==============================] - 21s 141us/sample - loss: 5.5603 - accuracy: 0.1066\n",
      "Epoch 6/100\n",
      "147278/147278 [==============================] - 21s 142us/sample - loss: 5.4632 - accuracy: 0.1123\n",
      "Epoch 7/100\n",
      "147278/147278 [==============================] - 21s 142us/sample - loss: 5.3504 - accuracy: 0.1191\n",
      "Epoch 8/100\n",
      "147278/147278 [==============================] - 21s 143us/sample - loss: 5.2530 - accuracy: 0.1248\n",
      "Epoch 9/100\n",
      "147278/147278 [==============================] - 21s 143us/sample - loss: 5.1616 - accuracy: 0.1283\n",
      "Epoch 10/100\n",
      "147278/147278 [==============================] - 21s 144us/sample - loss: 5.0705 - accuracy: 0.1337\n",
      "Epoch 11/100\n",
      "147278/147278 [==============================] - 21s 144us/sample - loss: 4.9901 - accuracy: 0.1380\n",
      "Epoch 12/100\n",
      "147278/147278 [==============================] - 21s 144us/sample - loss: 4.9119 - accuracy: 0.1409\n",
      "Epoch 13/100\n",
      "147278/147278 [==============================] - 21s 144us/sample - loss: 4.8364 - accuracy: 0.1448\n",
      "Epoch 14/100\n",
      "147278/147278 [==============================] - 22s 146us/sample - loss: 4.7625 - accuracy: 0.1493\n",
      "Epoch 15/100\n",
      "147278/147278 [==============================] - 21s 145us/sample - loss: 4.6971 - accuracy: 0.1523\n",
      "Epoch 16/100\n",
      "147278/147278 [==============================] - 21s 146us/sample - loss: 4.6424 - accuracy: 0.1543\n",
      "Epoch 17/100\n",
      "147278/147278 [==============================] - 21s 145us/sample - loss: 4.6030 - accuracy: 0.1562\n",
      "Epoch 18/100\n",
      "147278/147278 [==============================] - 22s 146us/sample - loss: 4.5923 - accuracy: 0.1575\n",
      "Epoch 19/100\n",
      "147278/147278 [==============================] - 22s 146us/sample - loss: 4.5385 - accuracy: 0.1611\n",
      "Epoch 20/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 4.5224 - accuracy: 0.1622\n",
      "Epoch 21/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 4.4726 - accuracy: 0.1660\n",
      "Epoch 22/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 4.4148 - accuracy: 0.1706\n",
      "Epoch 23/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 4.3516 - accuracy: 0.1765\n",
      "Epoch 24/100\n",
      "147278/147278 [==============================] - 22s 146us/sample - loss: 4.3162 - accuracy: 0.1793\n",
      "Epoch 25/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 4.2913 - accuracy: 0.1812\n",
      "Epoch 26/100\n",
      "147278/147278 [==============================] - 22s 146us/sample - loss: 4.2295 - accuracy: 0.1867\n",
      "Epoch 27/100\n",
      "147278/147278 [==============================] - 23s 153us/sample - loss: 4.1865 - accuracy: 0.1908\n",
      "Epoch 28/100\n",
      "147278/147278 [==============================] - 22s 150us/sample - loss: 4.1719 - accuracy: 0.1935\n",
      "Epoch 29/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 4.1302 - accuracy: 0.1974\n",
      "Epoch 30/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 4.0939 - accuracy: 0.2012\n",
      "Epoch 31/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 4.0939 - accuracy: 0.2017\n",
      "Epoch 32/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 4.0801 - accuracy: 0.2038\n",
      "Epoch 33/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 4.0449 - accuracy: 0.2069\n",
      "Epoch 34/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 3.9913 - accuracy: 0.2130\n",
      "Epoch 35/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 3.9499 - accuracy: 0.2182\n",
      "Epoch 36/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.9223 - accuracy: 0.2202\n",
      "Epoch 37/100\n",
      "147278/147278 [==============================] - 23s 156us/sample - loss: 3.9197 - accuracy: 0.2210\n",
      "Epoch 38/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.8648 - accuracy: 0.2268\n",
      "Epoch 39/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.8392 - accuracy: 0.2305\n",
      "Epoch 40/100\n",
      "147278/147278 [==============================] - 22s 147us/sample - loss: 3.8359 - accuracy: 0.2318\n",
      "Epoch 41/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.8208 - accuracy: 0.2336\n",
      "Epoch 42/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.8189 - accuracy: 0.2353\n",
      "Epoch 43/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.7832 - accuracy: 0.2388\n",
      "Epoch 44/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.7371 - accuracy: 0.2447\n",
      "Epoch 45/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.7016 - accuracy: 0.2486\n",
      "Epoch 46/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.6851 - accuracy: 0.2518\n",
      "Epoch 47/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.7442 - accuracy: 0.2463\n",
      "Epoch 48/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.7862 - accuracy: 0.2416\n",
      "Epoch 49/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.7979 - accuracy: 0.2406\n",
      "Epoch 50/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.7360 - accuracy: 0.2462\n",
      "Epoch 51/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.6810 - accuracy: 0.2531\n",
      "Epoch 52/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.6626 - accuracy: 0.2551\n",
      "Epoch 53/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.6410 - accuracy: 0.2579\n",
      "Epoch 54/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.5951 - accuracy: 0.2637\n",
      "Epoch 55/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.5751 - accuracy: 0.2671\n",
      "Epoch 56/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.5454 - accuracy: 0.2713\n",
      "Epoch 57/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.5507 - accuracy: 0.2709\n",
      "Epoch 58/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.4810 - accuracy: 0.2792\n",
      "Epoch 59/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.4325 - accuracy: 0.2856\n",
      "Epoch 60/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.4192 - accuracy: 0.2893\n",
      "Epoch 61/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.3833 - accuracy: 0.2939\n",
      "Epoch 62/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.3842 - accuracy: 0.2951\n",
      "Epoch 63/100\n",
      "147278/147278 [==============================] - 23s 153us/sample - loss: 3.3557 - accuracy: 0.2979\n",
      "Epoch 64/100\n",
      "147278/147278 [==============================] - 22s 152us/sample - loss: 3.3292 - accuracy: 0.3018\n",
      "Epoch 65/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.2913 - accuracy: 0.3083\n",
      "Epoch 66/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.2704 - accuracy: 0.3108\n",
      "Epoch 67/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.2856 - accuracy: 0.3100\n",
      "Epoch 68/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.2785 - accuracy: 0.3117\n",
      "Epoch 69/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.3548 - accuracy: 0.3044\n",
      "Epoch 70/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.2151 - accuracy: 0.3204\n",
      "Epoch 71/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 3.1910 - accuracy: 0.3236\n",
      "Epoch 72/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.1344 - accuracy: 0.3315\n",
      "Epoch 73/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.1067 - accuracy: 0.3371\n",
      "Epoch 74/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.0761 - accuracy: 0.3399\n",
      "Epoch 75/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.0451 - accuracy: 0.3459\n",
      "Epoch 76/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.0326 - accuracy: 0.3492\n",
      "Epoch 77/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 3.0173 - accuracy: 0.3517\n",
      "Epoch 78/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.9834 - accuracy: 0.3561\n",
      "Epoch 79/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.9818 - accuracy: 0.3577\n",
      "Epoch 80/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.9981 - accuracy: 0.3574\n",
      "Epoch 81/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.9680 - accuracy: 0.3603\n",
      "Epoch 82/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.9508 - accuracy: 0.3633\n",
      "Epoch 83/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.9978 - accuracy: 0.3574\n",
      "Epoch 84/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.8982 - accuracy: 0.3730\n",
      "Epoch 85/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.8563 - accuracy: 0.3767\n",
      "Epoch 86/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.8632 - accuracy: 0.3785\n",
      "Epoch 87/100\n",
      "147278/147278 [==============================] - 22s 148us/sample - loss: 2.8243 - accuracy: 0.3825\n",
      "Epoch 88/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.8271 - accuracy: 0.3839\n",
      "Epoch 89/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.8752 - accuracy: 0.3797\n",
      "Epoch 90/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.8886 - accuracy: 0.3776\n",
      "Epoch 91/100\n",
      "147278/147278 [==============================] - 24s 161us/sample - loss: 2.9020 - accuracy: 0.3768\n",
      "Epoch 92/100\n",
      "147278/147278 [==============================] - 22s 152us/sample - loss: 2.8532 - accuracy: 0.3823\n",
      "Epoch 93/100\n",
      "147278/147278 [==============================] - 23s 157us/sample - loss: 2.8503 - accuracy: 0.3833\n",
      "Epoch 94/100\n",
      "147278/147278 [==============================] - 24s 162us/sample - loss: 2.8443 - accuracy: 0.3858\n",
      "Epoch 95/100\n",
      "147278/147278 [==============================] - 23s 159us/sample - loss: 2.8487 - accuracy: 0.3854\n",
      "Epoch 96/100\n",
      "147278/147278 [==============================] - 23s 153us/sample - loss: 2.8200 - accuracy: 0.3889\n",
      "Epoch 97/100\n",
      "147278/147278 [==============================] - 23s 154us/sample - loss: 2.7822 - accuracy: 0.3948\n",
      "Epoch 98/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.7577 - accuracy: 0.3969\n",
      "Epoch 99/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.7373 - accuracy: 0.3998\n",
      "Epoch 100/100\n",
      "147278/147278 [==============================] - 22s 149us/sample - loss: 2.7557 - accuracy: 0.3991\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Embedding, LSTM\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load\n",
    "in_filename = 'trans_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they can do to keep us fighting with each other so that they can keep going to the bank you know how i describe the economic and social classes in this country the upper class keeps all of the money pays none of the taxes the middle class pays all of\n",
      "\n",
      "the taxes does you wear a good idea that would seem to sneeze and disappears the fuck out of jury a unit all looking for a few thousand nine and hell hangs on a zigzag religion had to wonder why i mean the word is huge dandy someone you know\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'trans_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"this morning after the results came in got a text from a friend of mine in london and she said the world feels like a safer place now that america has a new president and I said that’s great but America doesn’t do you guys remember what life was like before\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white and blue dick being jammed up their assholes every day because the owners of this country know the called the american dream because you have to be asleep to believe it pyramid of the hopeless but say what you want about american folks yeah you can say what you want\n",
      "\n",
      "to get out and shit swearing to me terrorism caring shit everywhere speaking of shit felt nuts or hopefully i travel after the kmart in the house and father but not healthy have to cover out of luck every thing you know what i mean i think ugly being a lot of people say to me is forward at all fuck you know what i mean that we have bombed the curtain and off strange than a unfortunate time in here you know what i mean i think beyond the household three fatigue four many hours cancer to be fair and it is extremely balls they are allimportant by state and try to send it to the media swimming off of the word a little bit and you know what valueless soulless continental thooom these people who suck the world trade center and having christsakes i wonder what the fuck is the ultimate makeover do you want to do is giving you a breakfast question about a serial killer up into beirut they have to compete with the floor and the inside tshirt in the first place of the brave honey it gives you a message to get a little\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'trans_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 200)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
