{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chappelle Monologue Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Jason Brownlee's Machine Learning Mastery: \"How to Develop a Word-Level Neural Language Model and Use it to Generate Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcripts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good people of Atlanta, we must never forget… that\n"
     ]
    }
   ],
   "source": [
    "print(transcript_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good people of Atlanta, we must never forget… that\n",
      "['good', 'people', 'of', 'atlanta', 'we', 'must', 'never', 'that', 'anthony', 'yeah', 'himself', 'anthony', 'bourdain', 'had', 'the', 'greatest', 'job', 'that', 'show', 'business']\n",
      "Total Tokens: 52922\n",
      "Unique Tokens: 4313\n",
      "Total Sequences: 52871\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"utf-8\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'transcripts.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:50])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'trans_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            215700    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4314)              435714    \n",
      "=================================================================\n",
      "Total params: 802,314\n",
      "Trainable params: 802,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52871 samples\n",
      "Epoch 1/100\n",
      "52871/52871 [==============================] - 10s 192us/sample - loss: 6.3165 - accuracy: 0.0422\n",
      "Epoch 2/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 5.9772 - accuracy: 0.0479\n",
      "Epoch 3/100\n",
      "52871/52871 [==============================] - 5s 96us/sample - loss: 5.8018 - accuracy: 0.0585\n",
      "Epoch 4/100\n",
      "52871/52871 [==============================] - 5s 96us/sample - loss: 5.6674 - accuracy: 0.0699\n",
      "Epoch 5/100\n",
      "52871/52871 [==============================] - 5s 97us/sample - loss: 5.5286 - accuracy: 0.0818\n",
      "Epoch 6/100\n",
      "52871/52871 [==============================] - 5s 96us/sample - loss: 5.4229 - accuracy: 0.0904\n",
      "Epoch 7/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 5.3422 - accuracy: 0.0994\n",
      "Epoch 8/100\n",
      "52871/52871 [==============================] - 5s 100us/sample - loss: 5.2639 - accuracy: 0.1072\n",
      "Epoch 9/100\n",
      "52871/52871 [==============================] - 5s 102us/sample - loss: 5.2626 - accuracy: 0.1068\n",
      "Epoch 10/100\n",
      "52871/52871 [==============================] - 6s 114us/sample - loss: 5.1449 - accuracy: 0.1169\n",
      "Epoch 11/100\n",
      "52871/52871 [==============================] - 5s 100us/sample - loss: 5.0649 - accuracy: 0.1224\n",
      "Epoch 12/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 4.9931 - accuracy: 0.1271\n",
      "Epoch 13/100\n",
      "52871/52871 [==============================] - 5s 102us/sample - loss: 4.9254 - accuracy: 0.1309\n",
      "Epoch 14/100\n",
      "52871/52871 [==============================] - 5s 98us/sample - loss: 4.8596 - accuracy: 0.1364\n",
      "Epoch 15/100\n",
      "52871/52871 [==============================] - 5s 100us/sample - loss: 4.7932 - accuracy: 0.1400\n",
      "Epoch 16/100\n",
      "52871/52871 [==============================] - 5s 100us/sample - loss: 4.7304 - accuracy: 0.1433\n",
      "Epoch 17/100\n",
      "52871/52871 [==============================] - 5s 99us/sample - loss: 4.6680 - accuracy: 0.1479\n",
      "Epoch 18/100\n",
      "52871/52871 [==============================] - 5s 98us/sample - loss: 4.6032 - accuracy: 0.1520\n",
      "Epoch 19/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 4.5415 - accuracy: 0.1558\n",
      "Epoch 20/100\n",
      "52871/52871 [==============================] - 5s 99us/sample - loss: 4.4775 - accuracy: 0.1584\n",
      "Epoch 21/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 4.4138 - accuracy: 0.1628\n",
      "Epoch 22/100\n",
      "52871/52871 [==============================] - 5s 102us/sample - loss: 4.3510 - accuracy: 0.1668\n",
      "Epoch 23/100\n",
      "52871/52871 [==============================] - 5s 99us/sample - loss: 4.2861 - accuracy: 0.1690\n",
      "Epoch 24/100\n",
      "52871/52871 [==============================] - 5s 100us/sample - loss: 4.2222 - accuracy: 0.1746\n",
      "Epoch 25/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 4.1591 - accuracy: 0.1779\n",
      "Epoch 26/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 4.0968 - accuracy: 0.1831\n",
      "Epoch 27/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 4.0402 - accuracy: 0.1874\n",
      "Epoch 28/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 3.9810 - accuracy: 0.1939\n",
      "Epoch 29/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 3.9204 - accuracy: 0.1998\n",
      "Epoch 30/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 3.8692 - accuracy: 0.2055\n",
      "Epoch 31/100\n",
      "52871/52871 [==============================] - 5s 104us/sample - loss: 3.8148 - accuracy: 0.2120\n",
      "Epoch 32/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 3.7671 - accuracy: 0.2166\n",
      "Epoch 33/100\n",
      "52871/52871 [==============================] - 5s 104us/sample - loss: 3.7157 - accuracy: 0.2225\n",
      "Epoch 34/100\n",
      "52871/52871 [==============================] - 5s 104us/sample - loss: 3.6697 - accuracy: 0.2289\n",
      "Epoch 35/100\n",
      "52871/52871 [==============================] - 5s 102us/sample - loss: 3.6203 - accuracy: 0.2367\n",
      "Epoch 36/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 3.5750 - accuracy: 0.2427\n",
      "Epoch 37/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 3.5317 - accuracy: 0.2483\n",
      "Epoch 38/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 3.4875 - accuracy: 0.2550\n",
      "Epoch 39/100\n",
      "52871/52871 [==============================] - 5s 104us/sample - loss: 3.4462 - accuracy: 0.2599\n",
      "Epoch 40/100\n",
      "52871/52871 [==============================] - 6s 104us/sample - loss: 3.4036 - accuracy: 0.2664\n",
      "Epoch 41/100\n",
      "52871/52871 [==============================] - 5s 101us/sample - loss: 3.3618 - accuracy: 0.2710\n",
      "Epoch 42/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 3.3221 - accuracy: 0.2757\n",
      "Epoch 43/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 3.2834 - accuracy: 0.2830\n",
      "Epoch 44/100\n",
      "52871/52871 [==============================] - 6s 106us/sample - loss: 3.2448 - accuracy: 0.2903\n",
      "Epoch 45/100\n",
      "52871/52871 [==============================] - 6s 109us/sample - loss: 3.2113 - accuracy: 0.2952\n",
      "Epoch 46/100\n",
      "52871/52871 [==============================] - 6s 108us/sample - loss: 3.1708 - accuracy: 0.3000\n",
      "Epoch 47/100\n",
      "52871/52871 [==============================] - 6s 117us/sample - loss: 3.1341 - accuracy: 0.3060\n",
      "Epoch 48/100\n",
      "52871/52871 [==============================] - 6s 121us/sample - loss: 3.0970 - accuracy: 0.3129\n",
      "Epoch 49/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 3.0626 - accuracy: 0.3182\n",
      "Epoch 50/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 3.0292 - accuracy: 0.3240\n",
      "Epoch 51/100\n",
      "52871/52871 [==============================] - 6s 107us/sample - loss: 2.9909 - accuracy: 0.3299\n",
      "Epoch 52/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 2.9605 - accuracy: 0.3361\n",
      "Epoch 53/100\n",
      "52871/52871 [==============================] - 5s 102us/sample - loss: 2.9251 - accuracy: 0.3394\n",
      "Epoch 54/100\n",
      "52871/52871 [==============================] - 6s 108us/sample - loss: 2.8918 - accuracy: 0.3464\n",
      "Epoch 55/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 2.8566 - accuracy: 0.3524\n",
      "Epoch 56/100\n",
      "52871/52871 [==============================] - 6s 107us/sample - loss: 2.8265 - accuracy: 0.3589\n",
      "Epoch 57/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 2.7947 - accuracy: 0.3631\n",
      "Epoch 58/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 2.7640 - accuracy: 0.3692\n",
      "Epoch 59/100\n",
      "52871/52871 [==============================] - 6s 107us/sample - loss: 2.7311 - accuracy: 0.3744\n",
      "Epoch 60/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 2.7042 - accuracy: 0.3780\n",
      "Epoch 61/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 2.6724 - accuracy: 0.3840\n",
      "Epoch 62/100\n",
      "52871/52871 [==============================] - 6s 104us/sample - loss: 2.6412 - accuracy: 0.3903\n",
      "Epoch 63/100\n",
      "52871/52871 [==============================] - 6s 108us/sample - loss: 2.6092 - accuracy: 0.3978\n",
      "Epoch 64/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 2.5818 - accuracy: 0.4015\n",
      "Epoch 65/100\n",
      "52871/52871 [==============================] - 6s 108us/sample - loss: 2.5552 - accuracy: 0.4059\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52871/52871 [==============================] - 6s 106us/sample - loss: 2.5224 - accuracy: 0.4127\n",
      "Epoch 67/100\n",
      "52871/52871 [==============================] - 6s 107us/sample - loss: 2.4941 - accuracy: 0.4175\n",
      "Epoch 68/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 2.4699 - accuracy: 0.4213\n",
      "Epoch 69/100\n",
      "52871/52871 [==============================] - 6s 106us/sample - loss: 2.4409 - accuracy: 0.4296\n",
      "Epoch 70/100\n",
      "52871/52871 [==============================] - 5s 104us/sample - loss: 2.4127 - accuracy: 0.4338\n",
      "Epoch 71/100\n",
      "52871/52871 [==============================] - 6s 110us/sample - loss: 2.3843 - accuracy: 0.4376\n",
      "Epoch 72/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 2.3690 - accuracy: 0.4416\n",
      "Epoch 73/100\n",
      "52871/52871 [==============================] - 6s 104us/sample - loss: 2.3338 - accuracy: 0.4471\n",
      "Epoch 74/100\n",
      "52871/52871 [==============================] - 6s 108us/sample - loss: 2.3014 - accuracy: 0.4535\n",
      "Epoch 75/100\n",
      "52871/52871 [==============================] - 6s 104us/sample - loss: 2.2821 - accuracy: 0.4568\n",
      "Epoch 76/100\n",
      "52871/52871 [==============================] - 6s 108us/sample - loss: 2.2536 - accuracy: 0.4628\n",
      "Epoch 77/100\n",
      "52871/52871 [==============================] - 6s 108us/sample - loss: 2.2277 - accuracy: 0.4686\n",
      "Epoch 78/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 2.2043 - accuracy: 0.4733\n",
      "Epoch 79/100\n",
      "52871/52871 [==============================] - 6s 106us/sample - loss: 2.1727 - accuracy: 0.4811\n",
      "Epoch 80/100\n",
      "52871/52871 [==============================] - 6s 106us/sample - loss: 2.1531 - accuracy: 0.4846\n",
      "Epoch 81/100\n",
      "52871/52871 [==============================] - 6s 104us/sample - loss: 2.1257 - accuracy: 0.4897\n",
      "Epoch 82/100\n",
      "52871/52871 [==============================] - 6s 107us/sample - loss: 2.1006 - accuracy: 0.4940\n",
      "Epoch 83/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 2.0823 - accuracy: 0.5005\n",
      "Epoch 84/100\n",
      "52871/52871 [==============================] - 6s 106us/sample - loss: 2.0519 - accuracy: 0.5044\n",
      "Epoch 85/100\n",
      "52871/52871 [==============================] - 6s 112us/sample - loss: 2.0278 - accuracy: 0.5092\n",
      "Epoch 86/100\n",
      "52871/52871 [==============================] - 6s 109us/sample - loss: 2.0050 - accuracy: 0.5137\n",
      "Epoch 87/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 1.9831 - accuracy: 0.5210\n",
      "Epoch 88/100\n",
      "52871/52871 [==============================] - 6s 109us/sample - loss: 1.9569 - accuracy: 0.5250\n",
      "Epoch 89/100\n",
      "52871/52871 [==============================] - 5s 104us/sample - loss: 1.9368 - accuracy: 0.5285\n",
      "Epoch 90/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 1.9101 - accuracy: 0.5340\n",
      "Epoch 91/100\n",
      "52871/52871 [==============================] - 6s 109us/sample - loss: 1.8941 - accuracy: 0.5375\n",
      "Epoch 92/100\n",
      "52871/52871 [==============================] - 15s 277us/sample - loss: 1.8700 - accuracy: 0.5435\n",
      "Epoch 93/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 1.8484 - accuracy: 0.5485\n",
      "Epoch 94/100\n",
      "52871/52871 [==============================] - 5s 103us/sample - loss: 1.8311 - accuracy: 0.5513\n",
      "Epoch 95/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 1.7934 - accuracy: 0.5582\n",
      "Epoch 96/100\n",
      "52871/52871 [==============================] - 6s 106us/sample - loss: 1.7751 - accuracy: 0.5648\n",
      "Epoch 97/100\n",
      "52871/52871 [==============================] - 6s 106us/sample - loss: 1.7585 - accuracy: 0.5668\n",
      "Epoch 98/100\n",
      "52871/52871 [==============================] - 6s 107us/sample - loss: 1.7351 - accuracy: 0.5720\n",
      "Epoch 99/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 1.7197 - accuracy: 0.5730\n",
      "Epoch 100/100\n",
      "52871/52871 [==============================] - 6s 105us/sample - loss: 1.7153 - accuracy: 0.5764\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Embedding, LSTM\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load\n",
    "in_filename = 'trans_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
